<!DOCTYPE html>
<html lang="en">
<head>
<link rel="icon" href="https://media.licdn.com/dms/image/D4E03AQF961Cgo3xCTQ/profile-displayphoto-shrink_200_200/0/1677295637294?e=1693440000&v=beta&t=kf9o1VsPJWK3Dd93fUJs7lryE-ID4FY_cSIfQOf87Zw">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
<title>Kathryn Yurechko</title>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<style>
* {
  box-sizing: border-box;
}

/* Style the body */
body {
  font-family: 'Lato';
}

/* Column container */
.row {  
  display: -ms-flexbox; /* IE10 */
  display: flex;
  -ms-flex-wrap: wrap; /* IE10 */
  flex-wrap: wrap;
  margin-left: 200px;
  margin-right: 200px;
  margin-top: 200px;
  margin-bottom: 200px;
}

/* Create two unequal columns that sits next to each other */
/* Sidebar/left column */
.side {
  -ms-flex: 40%; /* IE10 */
  flex: 40%;
  background-color: white;
  padding: 20px;
}

/* Main column */
.main {   
  -ms-flex: 60%; /* IE10 */
  flex: 60%;
  background-color: white;
  padding: 20px;
}

/* Responsive layout - when the screen is less than 700px wide, make the two columns stack on top of each other instead of next to each other */
@media screen and (max-width: 700px) {
  .row {   
    flex-direction: column;
  }
}

/* Responsive layout - when the screen is less than 400px wide, make the navigation links stack on top of each other instead of next to each other */
@media screen and (max-width: 400px) {
  .navbar a {
    float: none;
    width: 100%;
  }
}

footer {
    padding: 10px 20px;
    background-color: #D8BFD8;
    color: white;
    font-family: Lato, Helvetica, sans-serif;
    text-align: center;
    font-size: 12;
}
  
</style>
</head>
<body>
  
<div class="row">
  <div class="side">
    <h3>Marginalization through Content Moderation: An Interdisciplinary Analysis of the Silencing of Marginalized Voices on Social Media</h3>
    <h5>April 2023</h5>
  </div>
  <div class="main">
    <p><b>Introduction</b></p>
    <br>
    <p>Social media is largely considered a democratic space. It seemingly promotes the freedom and equality of all people by enabling individuals to voice their perspectives online, regardless of their identities. However, while many Americans use social media to share their opinions and experiences, algorithms disproportionately remove the content of users who hold marginalized identities without justification. Such unwarranted removals occur through the process of content moderation, whereby social media platforms attempt to rid the digital realm of content that violates their rules, including violent content and harassment. Cloaked in opacity, the silencing of marginalized users via content moderation not only reflects ongoing discrimination against marginalized groups, but it reinscribes systemic prejudices like racism and sexism into the physical world, often without public awareness. </p>
    <br>
    <p>Throughout my paper, I attempt to answer the following question: <em>What, if anything, should be done regarding social media content moderation policies and their enforcement to address the problems that they aim to address but without disadvantages for marginalized groups?</em> The threat that unfairness in content moderation poses to marginalized communities makes formulating and pursuing a response to this question into a moral imperative. Considering the silencing of marginalized individuals from a Rawlsian perspective, one understands that behind a veil of ignorance that prevents us from knowing our identity and personal circumstances, we would devise principles of justice that are not biased for or against any specific group. In particular, if someone did not know which racial or gender identity they would bear, they would choose to structure content moderation such that it would not harm the marginalized groups to which they could belong. This Rawlsian conception of justice as fairness enables us to acknowledge that we would not allow the silencing of voices that results in an inability of one’s interests to be heard, and therefore increases the potential for others to ignore or violate one’s interests, if we were ignorant of our own identities.  It is thus clear that Rawlsian justice requires us to intervene in unjust content moderation practices to reduce the silencing of individuals with marginalized identities online.</p>
    <br>
    <p>I structure my paper in three parts:</p>
    <p>i)	<em>Literature Review</em>: I describe social media content moderation, then explain the impacts of content moderation on marginalized groups as well as potential solutions to the negative impacts.</p>
    <p>ii) <em>Methodology</em>: I detail the approach that I will use to address my research question.</p>
    <p>iii)	<em>Analysis</em>: I evaluate the potential solutions that I identified in my Literature Review and ultimately propose my own solution to the disproportionate silencing of marginalized voices on social media.</p>
    <p>Finally, the <em>Conclusion</em> summarizes my paper, providing a holistic overview of the unfairness in content moderation in addition to my proposed solution to the problem.</p>
    <br>
    <p><b>Literature Review</b></p>
    <br>
    <p>To analyze and address issues in social media content moderation, particularly the disproportionate silencing of marginalized voices on social media platforms, one must first understand how content moderation on social media operates and impacts marginalized communities. This section in particular aims to examine three topics. First, I explain existing social media content moderation policies and enforcement mechanisms, which involve human content moderators, users, and algorithms. Second, I discuss the impacts of content moderation on marginalized communities as well as the causes behind these impacts. Finally, I describe the efforts that have been made with the potential to address content moderation biases. I explore each of these topics in the three subsections below.</p>
    <br>
    <p><b>Content Moderation Policies and Enforcement Mechanisms</b></p>
    <br>
    <p>Social media companies like Meta, which governs Facebook and Instagram, espouse missions of “giving people the power to build community and bring the world closer together" (2). Twitter and TikTok share similar goals “to give everyone the power to create and share ideas and information” (3) and “to inspire creativity and bring joy” (4), respectively. To facilitate their goals of fostering community-building and creative expression, these companies determine and enforce community guidelines that serve as standards of behavior on their platforms. The community guidelines and their enforcement mechanisms comprise the process of content moderation, which is “the organized practice of screening user-generated content (UGC)” to facilitate safe environments online (5).</p>
    <br>
    <p><b>I. Policies</b></p>
    <br>
    <p>Community guidelines are publicly accessible policies that outline problematic activities like hate speech, violence, and misinformation on social media platforms. A study by the Partnership for Countering Influence Operations (PCIO)  compiled and analyzed the community guidelines of thirteen social media and messaging platforms, including Facebook, Instagram, TikTok, and Twitter. The study found that while some platforms employ more generalized approaches, using vague language to describe prohibited activity, other platforms implement detailed policies that describe specific violations. Community guidelines additionally differ in their length and complexity, with Twitter’s guidelines totaling 23,110 words and detailing 29 policies while Instagram’s guidelines only total 208 words and detail 2 policies. <em>Generalized Policies</em> give platforms the flexibility to tweak and interpret their rules as they choose, whereas <em>Particularized Policies</em> are more inflexible but allow for greater transparency to users regarding what content is prohibited (6).</p>
    <br>
    <p>Beyond terminology, the substance of community guidelines differs by platform. Most platforms ban harassment and threats, spam, and violent content, but they tend to disagree about how to respond to child sexual abuse material, false information, and hate speech (7). For example, while TikTok clearly describes its understanding of and attempt to regulate hate speech (“Hateful ideologies are those that demonstrate clear hostility toward people because of their protected attributes. Hateful ideologies are incompatible with the inclusive and supportive community that our platform provides and we remove content that promotes them”(8)), Instagram simply writes, “We remove content that contains credible threats or hate speech” (9) without elaborating further.</p>
    <br>
    <p><b>II. Enforcement Mechanisms</b></p>
    <br>
    <p>The enforcement of community guidelines often relies on three mechanisms: human users, human content moderators, and algorithmic tools. Users of social media platforms flag or report content that they consider objectionable, which provides platforms with hand-selected, potentially inappropriate content for their human content moderators to review. These workers scroll minute-by-minute through that content and more, manually determining which content violates their platform’s community guidelines. Finally, platforms utilize artificial intelligence (AI) for content screening (10). For example, AI-backed content moderation can automatically review textual, audial, and visual content for community guidelines violations (11).</p>
    <br>
    <p>Inappropriate content can be identified during the upload process (i.e., when a user chooses to click “post”) or at any point after it is uploaded to a platform. Once inappropriate content is identified, users can experience a range of consequences, including content deletions, account bans, and shadow-banning, where content is made invisible to other users without actually being deleted from the platform. Users report that hindered content visibility or the loss of platform access not only hampers their capacity for self-expression; it impedes involvement in online communities and professional networks that underly social, political, and economic life. Though platforms allow users to appeal account bans and content removals that users believe were doled out in error, most users express that content moderation systems do not give them sufficient opportunity to channel their agency into challenging the consequences that had been assigned to them (9).</p>
    <br>
    <p><b>Understandings of Content Moderation Impacts on Marginalized Communities</b></p>
    <br>
    <p>In this paper, I define marginalized communities to be groups that experience social, political, or economic discrimination or exclusion due to historically unequal power dynamics within society. In this section, I explain the impacts of content moderation on marginalized communities and detail potential causes of these impacts. Though I will predominantly discuss the Black and LGBTQ+ (Lesbian, Gay, Bisexual, Transgender, and Queer) communities throughout this section, I acknowledge the diversity of communities that experience marginalization and the challenges that they might encounter in the digital realm.</p>
    <br>
    <p><b>I. Impacts</b></p>
    <br>
    <p>The #MoveMe guide to social movements and social media created by UC Berkeley outlines twenty-six movements that social media amplified by raising attention in the public consciousness. The paradigm example of social media facilitating social organization is the <em>Arab Spring</em>, also known as the <em>Arab Revolutions</em>, which utilized social media to spread information about pro-democracy efforts and gain support from a global audience. The ongoing <em>Black Lives Matter (BLM)</em> and <em>LGBTQ+ Rights</em> movements also reap similar benefits from social media. For example, BLM was catalyzed by Twitter in 2013 and gained greater momentum when a video of George Floyd being suffocated by police officers circulated social media platforms, forcing viewers to confront the realities of systematic racism that overwhelmingly impact the Black community in the United States. Today, the number of social media posts using the hashtag #BlackLivesMatter totals over forty million, which demonstrates the digital scope of the movement. In addition, the LGBTQ+ community gains awareness and visibility through social media, while its members learn about the community, find guidance, and share their lived experiences. Facebook in particular implemented options for LGBTQ+ support, allowing users to utilize the “rainbow flag reaction” to express support for LGBTQ+ media during Pride Month (12).</p>
    <br>
    <p>While social media strengthens the social movements of marginalized communities, with past U.S. Supreme Court Justice Anthony Kennedy stating that social media allows someone with Internet connectivity to “become a town crier with a voice that resonates farther than it could from any soapbox" (13), research shows that social media often silences or excludes marginalized users. For example, a 2021 news report revealed that Twitter’s image-cropping algorithm would reliably crop out Black people in pictures of Black and white people. For example, with photos that included Barack Obama and Mitch McConnell, the algorithm cropped the pictures to show only Mitch McConnell. Twitter then apologized and discontinued its image-cropping algorithm (14).</p>
    <br>
    <p>The majority of issues regarding the suppression of marginalized communities on social media involves content moderation. For example, The Washington Post references Facebook documents that detail the negative impacts of its previous race-blind model on Black users. The model regarded hate speech against all people equally, without considerations of race, which led “statements of contempt, inferiority, and disgust directed at White people and men” to comprise 90 percent of all hate speech that was removed by Facebook in 2020 (15). The Facebook algorithm has since been revised to prioritize the removal of hate speech against minorities (16). In addition, a Media Matters for America study in 2020 uncovered a bias against positive or neutral transgender content in favor of anti-trans posts on Facebook. The study found that 43% of posts about trans issues that earned at least 50,000 interactions involved negative attitudes toward trans athletes, while only 11% were about health care for trans youth (17). These examples show that social media algorithms perpetuate discrimination against the Black and LGBTQ+ communities.</p>
    <br>
    <p><b>II. Causes of Negative Impacts</b></p>
    <br>
    <p>Researchers provide three predominant explanations for the unjustified deletion of content related to marginalized communities. First, even though users report content that they find objectionable and human content moderators review it, the large volume of reports received each day as well as language subtleties (e.g., marginalized communities reclaiming words like the N-word) often prevent the accurate moderation of content, meaning that content which fails to violate a platform’s community guidelines nonetheless faces removal from the platform. In addition, reporting is often driven by partisanship and personal ideology, which can promote the elimination of unpopular speech. Second, platforms’ increasing reliance on AI for the identification of rule-violating content can automate biases in the content moderation process. For example, computational linguists found that tweets written in African-American Vernacular English (AAVE) commonly spoken by Black Americans are nearly twice as likely to be flagged as offensive compared to others. The insensitivity of content annotators to dialectical differences promoted this racial bias in automatic hate speech detection models, which were trained on annotator data (18). In summary, although natural language processing is often perceived as an objective tool to identify rule-violating content, algorithmic systems can misclassify content based on contextual details that might not be understood by the human beings who train the algorithms. Third, some researchers explain biases by pointing to platforms’ business models. For example, an AI ethicist describes, “Facebook profits from the proliferation of extremism, bullying, hate speech, disinformation, conspiracy theory, and rhetorical violence.” This is because more extreme content attracts users to spend more time engaging with a platform, thus raising platform revenues through more targeted advertising. For this reason, the ethicist concludes that “Facebook’s problem is not a technology problem. It is a business model problem" (19).</p>
    <br>
    <p><b>Solutions to Social Media Bias</b></p>
    <br>
    <p>A multiplicity of perspectives regarding the causes of content moderation biases produce a variety of advised solutions, which I divide into four categories: <em>fix the technology</em>, <em>reimagine the technology</em>, <em>fix the system at large</em>, and <em>reimagine the system at large</em>.</p>
    <br>
    <p><b>I. Fix the Technology</b></p>
    <br>
    <p>Technology teams often seek to remedy existing content moderation technologies, which involves tweaking the algorithmic processes that identify inappropriate content. For example, Stanford’s Artificial Intelligence Laboratory is working to develop a more inclusive content moderation algorithm for platforms like Reddit and Discord. The researchers shared, “We wanted to empower the people deploying machine learning models to make explicit choices about which voices their models reflect.” They explain that the Stanford algorithm is a “jury learning algorithm,” which simulates individual annotators whose data trains machine learning models. By modeling annotators, jury learning allows social media executives to mix and match different identities to comprise a fair and representative jury for evaluating content. The Stanford team also plans to build an ethical framework to guide executives in selecting a representative jury to govern the algorithm (20). In addition, European researchers champion a “human-is-the-loop” approach to semi-automated content moderation. Unlike the typical “human-in-the-loop” approach whereby algorithms at times consult human beings to judge the accuracy of their content moderation, the “human-is-the-loop” approach gives human content moderators ultimate control in deciding which content should or should not be removed from a platform (21).</p>
    <br>
    <p><b>II. Reimagine the Technology</b></p>
    <p>Some technologists take approaches besides improving existing content moderation algorithms, instead seeking to reimagine the standard approach to content moderation. For example, MIT researchers developed an experimental platform that gave users control over content moderation, which showed that users do evaluate content effectively and collaboratively (22). Their study proposed leveraging trusted-peer assessments to combat misinformation online, rather than rectifying the algorithms that already exist. The trusted-peer assessments would entail having each user indicate their trust in a subset of other users with whom they interact online; the user’s trusted connections would then rate the truthfulness of the user’s content when it is posted, thus enabling the content to be removed from the platform based on the cumulative judgement of trusted online connections (23). For example, if I am a trusted connection of a user who posts that the Earth is flat, I would give the user a very low accuracy rating, which could result in the removal of the user’s post depending on the assessments of their other trusted connections. This approach bears similarity to the process already adopted by Reddit. The platform uses community leader-moderators who enforce clear rules about what is prohibited from the platform and how violators will be punished. Each subreddit (i.e., micro-community within the Reddit platform that discusses a particular topic) operates under different rules, since leader-moderators specify rules that are relevant to the subreddit’s topical area. For example, the leader-moderator of a subreddit that focuses on video games might specify which forms of violence are acceptable (e.g., digital explosions) and which forms are unacceptable (e.g., digital murders). Leader-moderators then remove content that they determine to be in violation of the subreddit’s guidelines (24). As a result of this user-led process, volunteer content moderators on Reddit save the company $3.4 million worth of work every year (25).</p>
  </div>
</div>

<footer>
    <p>This site was built by &copy; Kathryn Yurechko, <a href="https://github.com/amyxzhang/personal_website">GitHub code here</a>.</p>
</footer>

</body>
</html>
